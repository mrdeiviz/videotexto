One of the best projects that you can work on
that has real legitimate potential
to make you a ton of money
has to do with web
scraping the ability
to collect real time data about travel, e-commerce,
health care,
real estate, you name
it is already a multi-billion dollar industry.
And you can tap into that with a project
like the one that I'm just about to show you.
The Potential
So first, let's discuss the potential.
Imagine you're a drop shipper or an e-commerce seller
and you're constantly competing against hundreds,
maybe even thousands of competitors.
Where your main differentiation is price
and maybe stock or availability.
If you had access to real time information
and were able to undercut your competitors prices
or raise your prices
when your competitors went low or out of stock.
Think about how much money
that could potentially make you.
Seriously, think about this for a second.
Do you have access to real time information?
This content $4 million business decisions.
And if you can provide that data to different clients
or people who can actually use it,
you can take a cut of those profits.
The Idea
Now, I won't lie to you.
This is not as trivial as I might make it sound.
Most companies are going to actively block
you from grabbing this type of information,
and even if they do provide
an API, you're
most likely going to deal with rate
limiting out-of-date information
and a whole bunch of other issues
that can completely ruin the whole point
of even having this system.
That's why you most likely need to do
is build a web scraper.
Now, that's exactly what I attempted
to do for this video.
I set out to build an automated web scraper
that would scan e-commerce
marketplaces for products of interest
for either me or my clients,
automatically tracking their prices
and alerting me
or my clients of any changes in those prices
such that we could react and modify our prices
so that we could take advantage of an opportunity.
The Process
Now, before I did all of this,
I did ask my community
if they had any advice on how to go
about building a project like this.
Well,
they suggested is to use a popular framework
like playwright or selenium.
Set up a web scraper.
Go to the Web site, scan for a specific product,
go pass
the different HTML, grab
all of the prices of that product, store
that in a database,
and then scan the database
and see if there's been any changes
since the last time we updated that product.
They then said I could run the scanner
automatically at a set increment.
So every day, every hour, every minute,
whatever increment I choose,
and that would be how we can work on this project.
The Problem
Now, unfortunately,
every time I attempted to do
this, my IP address got blocked.
I got stuck behind CAPTCHAs
and I just got completely denied
and shut out of the Web site
after just a few attempts at scraping it.
Now it turns out the websites are pretty smart.
They can detect bots
and they really don't want you scraping their website
and doing exactly what I was attempting to do.
Now, that's what I remembered, that about a year ago
I had worked with a company called Bright Data.
Now Bright Data essentially unblock
your browser for you.
It'll automatically solve CAPTCHAs
and all that kind of stuff.
So I reached out to them.
We connected
and they agreed to sponsor this video
so that I could continue building this project.
The Solution
Now a bright day to give
me access
to is something called their scraping browser,
which is a helpful browser
that works with puppeteer
selenium and play right
and automatically unblock websites for you.
It will connect to a proxy
network, rotate your IP address
and automatically solve CAPTCHAs,
essentially allowing
you unguided access into a website
and to perform web scraping.
Now, most importantly,
it also allows you to do scaling.
Meaning I could run
hundreds of instances of my scraper
at the exact same time,
and I wasn't limited to a single instance or whatever
my local machine could handle.
So that's what I ended up using for this project.
I'm going to show that to you in a second.
But if you do want to check out Bright Data,
they are the sponsor of this video.
You can check them out from the link
in the description
and you'll get access to some free credits
that you can try this out for yourself
and see the power of a browser like this.
Million Dollar Project Demo
All right.
So now let me show you the project
that I actually ended up building.
It's fully functioning and working right now.
If you want, you can extend this project.
I'm leaving it completely open source.
So click the GitHub link in the description
and feel free to do whatever you want With this code.
You can make something really cool.
This is kind of a solid base
and you could extend this
and make a legitimate business from.
Anyways, you can see that
what I have here is a product search tool. Really.
It's kind of analyzing e-commerce prices
and right now it goes to Amazon
and it will automatically scrape Amazon
every single day
for all of the different products
that I have that are set up here.
So I can either enable or disable
tracking of this product.
I can add a new product that I want to track
and then I can view
the individual prices for these products.
So let's click into Ryzen
nine 5950 X,
that's going to generate a table for us down here.
Of all of the information
we have on this product currently from Amazon dot K
Now you could
scrape multiple sources at once
for right now I've just set it up to do Amazon.
You'll see the way I wrote my code.
You can automatically add
other web scrapers and set up different sites.
Now it shows me all of the relevant listings
that contain the ryzen nine 5950 x processor
from Amazon gives me the current price
and any price change
since the last time this scraper rent.
So you can see down here
we have kind of a combo of a CPU and a motherboard
and this has gone up 0.76%
since the last time I ran this.
So you can see here we have this micro center AMD
whatever gives you all the information.
You can view the product URL,
view the source newest price time, current price,
and then it gives you a graph
actually telling you
the whole history of this product
from when this web scraper has been running.
Again, the way this works is every single day
it automatically scrapes all of your tracked products
and then gives you any updates on them.
And you're not always going to have a price change.
That depends on how frequently you run the scraper.
But in my case, I've seen a few different changes.
I was actually pretty surprised to find out
that even large sites like Amazon
are constantly updating their prices
and you can see
quite a few different price changes here.
So we'll go into one more
to see if there's any changes.
And something like Dove Men Shampoo.
Don't think we're going to see any here
just because this is a pretty basic kind of product
and I only actually ran the scraper one time,
so that makes sense.
We wouldn't see a price change there.
If I want, I can even track another product
so I can go up here and just search for one one time
or I can add it to my tracked product list
to be automatically updated ren every single day.
So let's just track a random product here.
Let's do something like Intel i9
ten 900 K
I don't know if that's recent or not,
but let's scrape this, okay,
we'll give that a second to update
and then I'll show you the table that it generates.
All right,
so this is finished,
so I'll click on this now
gives me the table
and shows me all of the different listings
from Amazon that were relevant that contained Intel.
I9 ten 900 K So I can then run this every single day
if I wanted to,
and I could add this into my tracked products list
if I wanted to automatically run this scraper on it
every few days.
And then again
I can kind of go here and toggle them on or off.
If I no longer want to track a specific product.
Obviously a lot more stuff you can do here.
But you see we have a very solid base setup.
So now let me hop over the code.
I'll give you a bit of information
on how this code functions.
So you have some insight
on how you could maybe change this
if you wanted to work on a project similar to this.
Code Walkthrough
All right.
So I've just hopped over to the code here
and I'm just going to give you
a very quick overview
of the architecture of this project.
And then again, feel free to look at the code
from the link in the description
and extend this as you see fit.
What I have for
this is kind of three
or four main components of a frontend
that's ready to react.
I have a backend that's written in flask with Python
and then I have the actual scraper,
which is a separate process.
I run from my backend
that's written in Python and uses playwright.
I also have an automation script which is very simple
and just automatically sends
a API request to my backend every day.
Now you could set this every hour, every minute.
You can use a cron job,
you can use all kinds of stuff,
but that's kind of how I've set this up.
So I'm just going to walk you
through very briefly the back end
and then the web scraper
and we'll kind of go from there.
Okay.
So in my back end, I have my app to py,
haven't organized this a ton.
And what I do is I set up here
or I connect to a local SQLite three database.
This is what I'm using to store all of my data.
Obviously you could use something
like Postgres or MongoDB,
but this was the simplest for this project.
I set up my different database tables here
using kind of the sequel, connector, SQL Alchemy,
whatever you want to call this and Flask.
And then I have a bunch of different endpoints.
Slash results
is actually where the web
scraper submits its results.
So the API really allows the web scraper to interact
kind of with my database and the backend.
So I send a post request to results
that submits the results
and then gets added to the database
and then the front end can view that data.
So that's what slash results is unique.
Search Text.
This is going to give all of the unique
kind of searches that we've done.
So the product names, essentially,
we then have a get request here for slash results,
which gives all of the results
for a specific product name,
which is the search text.
We then have all results
which will give all of the results
for all of the different products in our database.
We have start scraper,
which is a post request method,
which is going to start the web
scraper based on a URL in a specific search text.
We have ad tracked products straightforward.
We then have a put for a tracked product
which allows us to update it
so we can either toggle if we're tracking it or not,
then we can have the ability
to get all our tracked products
and then to update the tracked products.
What this will do
is automatically run
the web scraper on every single product instance.
So you can see what this does is
create a new some process where it
then runs our python scraper script,
which I'll go into now.
So I'm just going very quickly
kind of run through this,
but I have
this made up file
and what this does is kind of dynamically
allow you
to add different websites
so you could be scraping for
now I've just set it up with Amazon. Okay.
But you can connect this with really any website
you want,
so long as you write
a very kind of specific piece of logic
that will get the
kind of elements
you're looking for,
which I'll show you in one second.
Now, what I do here is I connect
right away to the bright data scraping browser.
It's actually extremely simple.
I was surprised how easy it was.
All I do is just have to have my username password
and then my credential,
which is what I get access to from my off JSON file,
which is inside of here.
And then I just use that as the URL in play, right?
For the browser I want to connect.
You also works with puppeteer and selenium,
but it's literally
like three or four lines of code
and you're automatically connected
this scalable network
where you can run multiple instances of this scraper.
So I'll really run through all of the text here,
but you can see
we fill the input field,
we press the search button,
we retrieve the products, we grab all of that,
and then eventually what we do is
we post the results here to our back end,
where they get submitted to the database.
Now the core logic here
is kind of connecting to the browser.
So you can see we're connecting over CDP
to this browser URL,
generate a new page, go to the URL,
you want to go to load the initial page, etc.
We then have Amazon dot py,
which is actually
what's responsible
for scraping the search page on Amazon.
Okay,
so it goes
in, gives us the image name price, you URL
get access to all of that
and then just return that
for each individual product.
Obviously there's a lot more logic
that's going on here, but that's the basics
and really everything that you need to know.
So that's kind of how this works.
Then I have my react front end
don't need to walk through that.
And then lastly,
I have this scheduler here
where all it does is just hit this URL here.
So update track products once a day.
The way this works is
I've set up a Windows
batch file that I put in my Windows
process scheduler,
where every day it's automatically
going to run this where it spins up the API
and then it will wait 10 seconds
and then call Python made up high,
which is right here,
which will send that post request
which will then go retrieve
all of the different products
which will be spun up in parallel
in separate processes
where we are scalable,
grabbing all of this data and adding it to the API.
BrightData Setup
Now just to quickly show you
in case you want to do this for yourself,
what you would need to do is go to Bright Data,
create a new account.
You can do that from the link in the description
and access the scraping browser.
I'll leave some links in the description,
but you can see it
kind of gives you some information here.
So from right data, go in.
I'm going to go to my proxies
and scraping infrastructure.
I have scraping browser right here.
I'll click on that.
You can see if I went to kind of the parameters here
would give me like the hostname
password, etc., which I don't want to share with you.
And that's really all you need.
Once you activate this
and you have access to that information,
you put that inside of the files,
you get your username, password,
password, host story,
and then you can just start using this.
There's really no more configuration required.
If we look at my stats here, you can see
I used it a ton
when I was actually developing this project
and then recently I've been running it
a few more times.
I believe you pay per gigabyte anyways, super, super
useful tool and again you get some free credits
if you want to check it out from the description.
Next Steps & Ideas
So that said, I think I'm going to wrap it up here.
The last thing I will mention
is that if you want to extend this project,
the next logical thing to do
would be to build an alert system
where what this will do is automatically
tell you in any of the prices change.
I haven't done that
just because I figured that be a nice thing
that you guys could work on if you want.
But what I would do to accomplish
this is just simply check.
Every time we submit new results.
If any of the prices have changed on the same product
since the last call,
if they have,
then we just grab all of those products,
throw them in some type of email
template, send an email to someone,
or we could send a text message,
do whatever you want.
With that said, I'm going to wrap up the video here.
I hope you guys found this helpful
and this gave you some inspiration
for a great project idea.
If it did leave, they like subscribe to the channel
and I will see you in the next one.
